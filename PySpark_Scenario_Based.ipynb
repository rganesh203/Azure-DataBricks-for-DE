{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNZJ6V8Qs0l+feHSj+KsDrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rganesh203/Pyspark/blob/main/PySpark_Scenario_Based.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. How to remove double quotes from a pyspark dataframe column using regex"
      ],
      "metadata": {
        "id": "5cz8nczwm4at"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['col1','col2']\n",
        "jsonString='{\"Zipcode\":\"74\",\"ZipCodeType\":\"STANDA\"RD\",\"City\":\"PARC PARQUE\",\"State\":\"PR\"}'\n",
        "\n",
        "rows = [(1,'abcd'),(2,jsonString)]\n",
        "\n",
        "df = spark.createDataFrame(rows,cols)\n",
        "\n",
        "df = df.withColumn(\"col3\",split(col(\"col2\"),'\"ZipCodeType\":\"')[0])\n",
        "df = df.withColumn(\"col4\",lit('\"ZipCodeType\":\"'))\n",
        "df = df.withColumn(\"col5\",split(col(\"col2\"),'\"ZipCodeType\":\"')[1])\n",
        "df = df.withColumn(\"col5\",concat_ws(\"\",split(split(col(\"col2\"),'\"ZipCodeType\":\"')[1],'\"',2)))\n",
        "\n",
        "df = df.withColumn(\"col6\", concat(col(\"col3\"),col(\"col4\"),col(\"col5\"))).select(col(\"col2\"),col(\"col6\"))\n",
        "display(df)"
      ],
      "metadata": {
        "id": "0UwR6cbilrs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. PySpark Check if Column Exists in DataFrame"
      ],
      "metadata": {
        "id": "TG4i1aYxpOiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Method1\n",
        "listColumns=df.schema.fieldNames()\n",
        "if fieldNames.count('id')>0:\n",
        "    print('id column is present')\n",
        "else:\n",
        "    print('id column is  not present')\n",
        "\"colum_name\"  in listColumns"
      ],
      "metadata": {
        "id": "X4Xnr2SgpRnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method2\n",
        "\n",
        "listColumns=df.columns\n",
        "\"colum_name\"  in listColumns"
      ],
      "metadata": {
        "id": "qUz_ZDqfqMbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Convert PySpark DataFrame to Pandas"
      ],
      "metadata": {
        "id": "lQ5Jy-NqqXG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
        "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
        "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
        "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
        "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
        "\n",
        "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
        "pysparkDF = spark.createDataFrame(data = data, schema = columns)\n",
        "pysparkDF.printSchema()\n",
        "pysparkDF.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "UfEV_cE0qY5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pandasDF = pysparkDF.toPandas()\n",
        "print(pandasDF)\n"
      ],
      "metadata": {
        "id": "jt36qlCzqfmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Different ways to apply function on Column in Dataframe using PySpark"
      ],
      "metadata": {
        "id": "Orlv2poIqhk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "columns = [\"Seqno\",\"Name\"]\n",
        "data = [(\"1\", \"john jones\"),\n",
        "    (\"2\", \"tracey smith\"),\n",
        "    (\"3\", \"amy sanders\")]\n",
        "\n",
        "df = spark.createDataFrame(data=data,schema=columns)\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Apply function using withColumn\n",
        "from pyspark.sql.functions import upper\n",
        "df.withColumn(\"Upper_Name\", upper(df.Name)) \\\n",
        "  .show()\n",
        "\n",
        "# Apply function using select\n",
        "df.select(\"Seqno\",\"Name\", upper(df.Name)) \\\n",
        "  .show()\n",
        "\n",
        "# Apply function using sql()\n",
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"select Seqno, Name, UPPER(Name) from TAB\") \\\n",
        "     .show()\n",
        "\n",
        "# Create custom function\n",
        "def upperCase(str):\n",
        "    return str.upper()\n",
        "\n",
        "# Convert function to udf\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType\n",
        "upperCaseUDF = udf(lambda x:upperCase(x),StringType())\n",
        "\n",
        "# Custom UDF with withColumn()\n",
        "df.withColumn(\"Cureated Name\", upperCaseUDF(col(\"Name\"))) \\\n",
        "  .show(truncate=False)\n",
        "\n",
        "# Custom UDF with select()\n",
        "df.select(col(\"Seqno\"), \\\n",
        "    upperCaseUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
        "   .show(truncate=False)\n",
        "\n",
        "# Custom UDF with sql()\n",
        "spark.udf.register(\"upperCaseUDF\", upperCaseUDF)\n",
        "df.createOrReplaceTempView(\"TAB\")\n",
        "spark.sql(\"select Seqno, Name, upperCaseUDF(Name) from TAB\") \\\n",
        "     .show()\n"
      ],
      "metadata": {
        "id": "s4qeFqX6r9Nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you have learned how to apply a built-in function to a PySpark column by using withColumn(), select() and spark.sql(). Also learned how to create a custom UDF function and apply this function to the column."
      ],
      "metadata": {
        "id": "ZOoz4r1Xrrae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. printSchema() to string or json in PySpark"
      ],
      "metadata": {
        "id": "r9bOpVqCsYG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession\n",
        "spark = SparkSession.builder.master(\"local[1]\") \\\n",
        "                    .appName('SparkByExamples.com') \\\n",
        "                    .getOrCreate()\n",
        "\n",
        "# Create DataFrame\n",
        "columns = [\"language\",\"fee\"]\n",
        "data = [(\"Java\", 20000), (\"Python\", 10000), (\"Scala\", 10000)]\n",
        "\n",
        "df = spark.createDataFrame(data).toDF(*columns)\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "5X-GWJ6_sZXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=df.schema.simpleString()\n",
        "print(a)\n",
        "b=df.schema.json()\n",
        "print(b)"
      ],
      "metadata": {
        "id": "kS5xblzLsxPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. How to Write Dataframe as single file with specific name in PySpark"
      ],
      "metadata": {
        "id": "9vqeUl6EtXS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Single file using Spark coalesce() & repartition()\n",
        "When you are ready to write a DataFrame, first use Spark repartition() and coalesce() to merge data from all partitions into a single partition and then save it to a file."
      ],
      "metadata": {
        "id": "ihjcfZK3uLik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "def write_csv_with_specific_file_name(sc, df, path, filename):\n",
        "    file_format =\n",
        "    df.repartition(1).write.option(\"header\", \"true\").format(file_format).save(path)\n",
        "    try:\n",
        "        sc_uri = sc._gateway.jvm.java.net.URI\n",
        "        sc_path = sc._gateway.jvm.org.apache.hadoop.fs.Path\n",
        "        file_system = sc._gateway.jvm.org.apache.hadoop.fs.FileSystem\n",
        "        configuration = sc._gateway.jvm.org.apache.hadoop.conf.Configuration\n",
        "        fs = file_system.get(sc_uri(\"hdfs://{HDFS_IP/NAME}\"), configuration())\n",
        "        src_path = None\n",
        "        status = fs.listStatus(sc_path(path))\n",
        "        for fileStatus in status:\n",
        "            temp = fileStatus.getPath().toString()\n",
        "            if \"part\" in temp:\n",
        "                src_path = sc_path(temp)\n",
        "        dest_path = sc_path(path + filename)\n",
        "        if fs.exists(src_path) and fs.isFile(src_path):\n",
        "            fs.rename(src_path, dest_path)\n",
        "            fs.delete(src_path, True)\n",
        "    except Exception as e:\n",
        "        raise Exception(\"Error renaming the part file to {}:\".format(filename, e))\n",
        "\n",
        "\n",
        "def create_dataframe(spark):\n",
        "    simple_data = [(\"James\", \"Sales\", \"NY\", 90000),\n",
        "                   (\"Michael\", \"Sales\", \"NY\", 86000),\n",
        "                   (\"Robert\", \"Sales\", \"CA\", 81000),\n",
        "                   (\"Maria\", \"Finance\", \"CA\", 90000),\n",
        "                   (\"Raman\", \"Finance\", \"CA\", 99000),\n",
        "                   (\"Scott\", \"Finance\", \"NY\", 83000),\n",
        "                   (\"Jen\", \"Finance\", \"NY\", 79000),\n",
        "                   (\"Jeff\", \"Marketing\", \"CA\", 80000),\n",
        "                   (\"Kumar\", \"Marketing\", \"NY\", 91000)\n",
        "                   ]\n",
        "\n",
        "    schema = [\"employee_name\", \"department\", \"state\", \"salary\"]\n",
        "    return spark.createDataFrame(data=simple_data, schema=schema)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    df = create_dataframe(spark)\n",
        "    write_csv_with_specific_file_name(spark.sparkContext, df, \"hdfs://cluster_name/path/to/destination\", \"/keep_this_file_name.csv\")"
      ],
      "metadata": {
        "id": "XMYBmJF2tZob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.multiple delimiters pyspark"
      ],
      "metadata": {
        "id": "z2QesmL-vabU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession,types\n",
        "spark= SparkSession.builder.master(\"local\").appName('multiple_delimiter').getOrCreate()\n",
        "df=spark.read.text('D:\\python_coding\\pyspark_tutorial\\multiple_delimiter.csv')\n",
        "df.show(truncate=0)"
      ],
      "metadata": {
        "id": "bzV_nU4nvcKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header=df.first([0])\n",
        "header"
      ],
      "metadata": {
        "id": "_7oPdNjNwJlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema=header.split('~|')\n",
        "schema"
      ],
      "metadata": {
        "id": "_yThDG2LxDA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1df.filter(df['value']!= header).rdd.map(lambda x: x[0].split('~|')).toDF(schema)"
      ],
      "metadata": {
        "id": "kbYJcfgyxMmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1.show(truncate=0)"
      ],
      "metadata": {
        "id": "WVqLYoqeyJun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Merge DataFrame in Spark"
      ],
      "metadata": {
        "id": "rJ3U4hITyGth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1=spark.read.option(\"delimeter\",\"|\").csv('input.csv')\n",
        "df1.show()"
      ],
      "metadata": {
        "id": "ZW6sb_KwyqEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=spark.read.option(\"delimeter\",\"|\").csv('input2.csv',header=True)\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "tQX9kQkC0bwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#add new_column in df1\n",
        "from pyspark.sql.functions import lit\n",
        "df_add=df.withColumn(\"bonus_percent\", lit('null'))\n",
        "df_add.show()"
      ],
      "metadata": {
        "id": "kMDf2tzH0rBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Method1\n",
        "df_add.union(df2).show()"
      ],
      "metadata": {
        "id": "pKSmE3Pg1dt1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method2\n",
        "from pyspark.sql.types import *\n",
        "schema = StructType(\n",
        "    [\n",
        "    StructField(\"name\",StringType(),True),\n",
        "    StructField(\"age\",StringType(),True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "  ]\n",
        "  )"
      ],
      "metadata": {
        "id": "8Ci-Do2g1pOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3=spark.read.option(\"delimeter\",\"|\").csv('input.csv',header=True,schema=schema)\n",
        "df4=spark.read.option(\"delimeter\",\"|\").csv('input2.csv',header=True,schema=schema)\n",
        "df3.union(df4).show()"
      ],
      "metadata": {
        "id": "U6fFZ9h12lxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outer join\n",
        "f_df=spark.read.option(\"delimeter\",\"|\").csv('input.csv',header=True)\n",
        "s_df=spark.read.option(\"delimeter\",\"|\").csv('input2.csv',header=True)\n",
        "df_ot=f_df.join(s_df,on=['name','age'], how=\"Outer\")\n",
        "df_ot.show()"
      ],
      "metadata": {
        "id": "59UNCfjS3JTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#method4"
      ],
      "metadata": {
        "id": "LUHJKUNV4Mmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=spark.read.option(\"delimeter\",\"|\").csv('input.csv',header=True)\n",
        "df2=spark.read.option(\"delimeter\",\"|\").csv('input2.csv',header=True)"
      ],
      "metadata": {
        "id": "vkc0zywa4PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lista=list(set(df1.columns)-set(df2.columns))\n",
        "listb=list(set(df2.columns)-set(df1.columns))"
      ],
      "metadata": {
        "id": "AWYN0J7v4TTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in lista:\n",
        "    df2=df2.withcolumn(i,lit(\"null\"))\n",
        "for i in lista:\n",
        "    df1=df1.withcolumn(i,lit(\"null\"))"
      ],
      "metadata": {
        "id": "Kd5wB9EG4l75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.union(df2).show()"
      ],
      "metadata": {
        "id": "1Futq2Sa0axV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. PySpark Out of Memory Issue"
      ],
      "metadata": {
        "id": "Cb7_-Bz_zwRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OutOfMemory error can occur here due to incorrect usage of Spark. The driver in the Spark architecture is only supposed to be an orchestrator and is therefore provided less memory than the executors. You should always be aware of what operations or tasks are loaded to your driver."
      ],
      "metadata": {
        "id": "Pu4wibEdzuL1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Driver OOM\n",
        "2.Executer OOM"
      ],
      "metadata": {
        "id": "wNDapVzZ-K0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Driver Memory Issues\n",
        "    collect\n",
        "    Broadcast Huge Data\n",
        "Executer Memory Issues\n",
        "    Big Partition\n",
        "    Yarn Memory Overhead\n",
        "    High Concurrency"
      ],
      "metadata": {
        "id": "zpgaxLhE-S8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10. Spark Executor Tuning | Decide Number Of Executors and Memory"
      ],
      "metadata": {
        "id": "M5flUAfaAjUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Resources we have\n",
        "    6 Machines\n",
        "    16 cores (Total cores in cluster 16*6=96)\n",
        "    64 GB Ram/Machine"
      ],
      "metadata": {
        "id": "6nmOo3S9AmLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "our options\n",
        "    smalle size executore\n",
        "    biggest size executors"
      ],
      "metadata": {
        "id": "yUNvYmykBWK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "smallest executors\n",
        "11 core and 4 GB Ram per executors\n",
        "16 executors on each machine\n",
        "but we are not using executors cores\n"
      ],
      "metadata": {
        "id": "8mFFspZKBXPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Largest executor\n",
        "    6 executor\n",
        "    64 GB ram per executor\n",
        "    16 cores per executor\n",
        "    IO contention\n",
        "    no resources for OS\n",
        "    No memry overhead for YARN.\n",
        "    \n"
      ],
      "metadata": {
        "id": "tKYTSfeVBssJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correct way\n",
        "executors and executor core\n",
        "    -no of cores: 96-no of machines=90\n",
        "    -no of cores per machine: 90/6=15\n",
        "    -no of executors cores:5\n",
        "    -so no executors per machine:15/5=3\n",
        "memory:\n",
        "    -available per machine:63GB\n",
        "    -available per executor: 63/321 GB\n",
        "    YARN over Haed: 2GB\n",
        "    per Executor Memory: 19 GB"
      ],
      "metadata": {
        "id": "P1rBcL2jCedn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Avro vs Parquet"
      ],
      "metadata": {
        "id": "BVF-mVCwEIHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avro differs from ORC and Parquet in that it uses a row-based, rather than column-based storage configuration.\n",
        "\n",
        "Avro uses JSON for defining data types and protocols so it's easy to read and interpret."
      ],
      "metadata": {
        "id": "3uOcnrdREKTO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#12.partitioning vs bucketing"
      ],
      "metadata": {
        "id": "2SptfZrTEQAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Partitioning helps in elimination of data, if used in WHERE clause, where as bucketing helps in organizing data in each partition into multiple files, so as same set of data is always written in same bucket."
      ],
      "metadata": {
        "id": "Ju7kgvt7EaXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13. Dealing with Date in PySpark"
      ],
      "metadata": {
        "id": "hkSPspsaz8fA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import finsaprk\n",
        "findspark.init()\n"
      ],
      "metadata": {
        "id": "WGtR9Jkf054H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession,types\n",
        "spark= SparkSession.builder.master(\"local\").appName('date demo').getOrCreate()\n",
        "df=spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(\"dem13.csv\")\n",
        "df.printSchema()"
      ],
      "metadata": {
        "id": "F88iAXHl0xSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "FhdMNUC91eUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import date_add,to_date,col,expr\n",
        "df.select(to_date(col(\"ReachargeDate\").cast(\"string\",\"yyyyMMdd\"))).show()"
      ],
      "metadata": {
        "id": "T3lwm1yV1hpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_s=df.witcolumn(\"date_s\",to_date(col(\"ReachargeDate\").cast(\"string\",\"yyyyMMdd\"))"
      ],
      "metadata": {
        "id": "UEQsxQ-O2VWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_s.printSchema()"
      ],
      "metadata": {
        "id": "XJHf8-X42zFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_s.select('*',expr(\"date_add(date_s,remaining_daye)\"))"
      ],
      "metadata": {
        "id": "doA-8l9u26Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sort by vs order by"
      ],
      "metadata": {
        "id": "lDgYGQ6Jq7CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. difference between sort by and order by\n",
        "Order By and Sort By both are not same in sql. Order by will do sorting an entire data. sort by will do partition wise sorting\n",
        "orderBy and sort both are same in pyspark. sortWintinPartitions as same as sort By in sql."
      ],
      "metadata": {
        "id": "cXYJffhcw7Rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. Bad Records Handling in databricks"
      ],
      "metadata": {
        "id": "pyAhjmtb8Ccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#In Databricks, Bad Records Handling refers to the process of dealing with invalid or erroneous data encountered during ETL (Extract, Transform, Load) or data processing pipelines. When working with large datasets and complex data transformations, it is common to encounter bad records, which are data rows that do not conform to the expected schema or fail certain validation rules.\n",
        "\n",
        "#Databricks provides several approaches to handle bad records during data processing:\n",
        "\n",
        "#Fail Fast:\n",
        "\n",
        "#This is the default behavior in Databricks. If any bad record is encountered during processing, the job fails immediately, and an error message is generated. This approach is suitable when you want to be alerted immediately about data issues.\n",
        "#Drop Malformed Rows:\n",
        "\n",
        "#With this approach, you can instruct Databricks to drop the rows that contain bad data, and the data processing continues for the rest of the valid records. This is achieved by using the .option(\"mode\", \"DROPMALFORMED\") option when reading data.\n",
        "#Permissive Mode:\n",
        "\n",
        "#Permissive mode is an option that allows Databricks to process bad records while attempting to infer the schema automatically. Invalid records are converted to null or other default values according to the data type. This is useful when dealing with semi-structured or messy data where the schema might not be well-defined.\n",
        "#Custom Schema and Error Handling:\n",
        "\n",
        "#Databricks allows you to provide a custom schema when reading data. By specifying a custom schema, you can control the data types and handling of bad records explicitly. Additionally, you can use the .onCorrupt option to define a custom action or error handling logic for dealing with corrupt or bad records.\n",
        "#Here's an example of how to use the option and onCorrupt options in Databricks:\n",
        "\n",
        "# Using Drop Malformed Rows\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"DROPMALFORMED\").load(\"path_to_data\")\n",
        "\n",
        "# Using Permissive Mode\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\").load(\"path_to_data\")\n",
        "\n",
        "# Using Custom Schema and Error Handling\n",
        "from pyspark.sql.types import StructType, StringType, IntegerType\n",
        "\n",
        "customSchema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    # Add more fields as per your data schema\n",
        "])\n",
        "\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\").schema(customSchema).load(\"path_to_data\")\n",
        "#By choosing the appropriate approach for bad records handling in Databricks, you can ensure that your data processing pipelines are robust, and the output is of high quality, even when dealing with imperfect data."
      ],
      "metadata": {
        "id": "vkk6gIF87rYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#16. cache and persist"
      ],
      "metadata": {
        "id": "b8My1WNpE5ks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''In PySpark, both cache and persist are used to persist intermediate DataFrame or RDD (Resilient Distributed Dataset) results in memory or on\n",
        "disk to avoid recomputation and improve performance during iterative or interactive data processing tasks.\n",
        "\n",
        "cache is a shorthand for persist() with the default storage level MEMORY_AND_DISK.\n",
        "When you use cache, the DataFrame or RDD is stored in memory as well as on disk, making it readily available for quick access in subsequent operations.\n",
        "The data is stored in memory and spilled to disk if the memory is not sufficient to hold the entire dataset.\n",
        "The storage level can be explicitly specified using the StorageLevel argument.\n",
        "Example:'''\n",
        "\n",
        "# Caching a DataFrame with the default storage level (MEMORY_AND_DISK)\n",
        "df.cache()\n",
        "'''persist:\n",
        "persist allows you to specify the storage level explicitly.\n",
        "You can choose from various storage levels based on your specific use case and available resources, such as MEMORY_ONLY, MEMORY_AND_DISK, DISK_ONLY,\n",
        "OFF_HEAP, etc.\n",
        "Using persist, you have more control over where and how the data is stored.\n",
        "Example:'''\n",
        "\n",
        "from pyspark.storagelevel import StorageLevel\n",
        "\n",
        "# Persisting a DataFrame with MEMORY_ONLY storage level\n",
        "df.persist(StorageLevel.MEMORY_ONLY)\n",
        "#Which one to use?\n",
        "\n",
        "'''If you want a simple and quick way to cache a DataFrame in memory with a default storage level, cache is a convenient choice.\n",
        "If you need to customize the storage level or choose a specific storage strategy (e.g., storing on disk only or off-heap), you can use persist.\n",
        "Considerations:\n",
        "\n",
        "Be cautious while using cache or persist as they consume memory resources. If you cache or persist too many DataFrames or RDDs, it may lead to excessive\n",
        "memory usage and eviction of other important data from memory.\n",
        "Make sure to unpersist DataFrames or RDDs that are no longer needed to free up memory.\n",
        "In summary, both cache and persist are used for data persistence in PySpark, with cache being a convenient shorthand for the most common use case. Use\n",
        "persist when you need to customize the storage level or storage strategy based on your specific data processing requirements. Always be mindful of memory\n",
        "usage and free up unnecessary persisted data to optimize memory utilization.'''"
      ],
      "metadata": {
        "id": "ioR0sSyVE-zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "10 trending questions asked in Apache Spark interviews\n",
        "\n",
        "1. how are initial number of partitions calculated in a dataframe\n",
        "\n",
        "2. what happens internally when you execute spark-submit\n",
        "\n",
        "3. what is a partition skew and how to tackle it\n",
        "\n",
        "4. what are the spark optimization techniques you have used\n",
        "\n",
        "5. what is a broadcast join, how does it work internally\n",
        "\n",
        "6. how do you optimize 2 large table joins\n",
        "\n",
        "7. please explain about memory management in apache spark\n",
        "\n",
        "8. what is caching in spark, and when do you consider caching a dataframe\n",
        "\n",
        "9. how do you handle out of memory errors in spark\n",
        "\n",
        "10. what is the difference between partitioning and bucketing, please explain with a usecase"
      ],
      "metadata": {
        "id": "IOdcUg_Stz8S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In Apache Spark, the initial number of partitions in a DataFrame is determined during the data loading phase when the DataFrame is created from an external data source, such as reading data from a file or connecting to a database. The number of partitions is influenced by various factors and can vary based on the data source and configuration.\n",
        "\n",
        "Here are some factors that can affect the initial number of partitions:\n",
        "\n",
        "Default Parallelism: By default, Spark tries to set the number of partitions equal to the number of cores available on the cluster. This is known as the default parallelism.\n",
        "\n",
        "Input Data Size: The size of the data being loaded can also impact the number of partitions. If the data size is significant, Spark may create more partitions to parallelize data processing and leverage the cluster's resources effectively.\n",
        "\n",
        "Block Size (for HDFS): If you are reading data from HDFS (Hadoop Distributed File System), the HDFS block size can influence the number of partitions. The default block size is typically 128 MB, and Spark tries to create partitions that align with these blocks to improve data locality.\n",
        "\n",
        "Configuration Settings: You can explicitly set the number of partitions using configuration settings in Spark. For example, when reading data from a file, you can specify the numPartitions parameter to control the number of partitions.\n",
        "\n",
        "Example:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "# Setting the number of partitions explicitly when reading a CSV file\n",
        "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file.csv\", numPartitions=10)\n",
        "Data Source Specifics: Different data sources may have their own strategies for determining the initial number of partitions. For instance, when reading data from a database, the JDBC driver might provide a default value for the number of partitions.\n",
        "\n",
        "Spark Version: The behavior of partitioning can also change with different Spark versions, as the underlying code and optimizations evolve.\n",
        "\n",
        "Keep in mind that the initial number of partitions is just a starting point, and Spark can dynamically repartition or coalesce data during transformations, depending on the operations you perform on the DataFrame. For example, when using operations like groupBy, join, or repartition, Spark can shuffle and repartition the data to optimize data processing.\n",
        "\n",
        "You can always check the current number of partitions in a DataFrame using the getNumPartitions() method:\n",
        "\n",
        "python\n",
        "Copy code\n",
        "num_partitions = df.rdd.getNumPartitions()\n",
        "Overall, the initial number of partitions in a DataFrame is a dynamic process influenced by various factors, but it is essential to understand and consider it when working with large datasets and aiming for optimal data processing performance."
      ],
      "metadata": {
        "id": "1T0AQbZCuHyx"
      }
    }
  ]
}