{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4284534-f333-4581-8aea-c6e64945bf4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------+\n| id|value|cumulative_sum|\n+---+-----+--------------+\n|  A|   10|            10|\n|  B|   20|            30|\n|  C|   30|            60|\n|  D|   40|           100|\n+---+-----+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Create a Spark session\n",
    "\n",
    "# Sample DataFrame creation\n",
    "data = [(\"A\", 10),\n",
    "        (\"B\", 20),\n",
    "        (\"C\", 30),\n",
    "        (\"D\", 40)]\n",
    "\n",
    "columns = [\"id\", \"value\"]\n",
    " \n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Define a Window specification\n",
    "window_spec = Window.orderBy(\"id\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Add a new column for cumulative sum\n",
    "df = df.withColumn(\"cumulative_sum\", F.sum(\"value\").over(window_spec))\n",
    "\n",
    "# Show the result\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "951ead8e-6f4b-4e25-b54f-b6f7d906bf47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+\n|employee_name|salary|\n+-------------+------+\n|          Eve|   180|\n+-------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Sample DataFrame creation\n",
    "data = [(\"Alice\", 100),\n",
    "        (\"Bob\", 150),\n",
    "        (\"Charlie\", 120),\n",
    "        (\"David\", 200),\n",
    "        (\"Eve\", 180)]\n",
    "\n",
    "columns = [\"employee_name\", \"salary\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Specify the desired rank\n",
    "nth_rank = 2  # Change this to the desired rank\n",
    "\n",
    "# Define a Window specification ordering by salary in descending order\n",
    "window_spec = Window.orderBy(F.desc(\"salary\"))\n",
    "\n",
    "# Add a new column with the row number based on the window specification\n",
    "df = df.withColumn(\"salary_rank\", F.row_number().over(window_spec))\n",
    "\n",
    "# Filter the DataFrame to get the nth highest salary\n",
    "result = df.filter(F.col(\"salary_rank\") == nth_rank).select(\"employee_name\", \"salary\")\n",
    "\n",
    "# Show the result\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef82ec22-4fee-4aff-9978-4c5778270e6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana#cherry#orange']\n"
     ]
    }
   ],
   "source": [
    "txt = \"apple#banana#cherry#orange\"\n",
    "\n",
    "# setting the maxsplit parameter to 1, will return a list with 2 elements!\n",
    "x = txt.split(\"#\", 1)\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "794165ce-da02-4669-ad48-84c6c742f1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>c01</th><th>c02</th></tr></thead><tbody><tr><td>2</td><td>{\"Id\":\"2\",\"Name\":\"Maheer:Basha\", \"City\": \"Hyd\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "{\"Id\":\"2\",\"Name\":\"Maheer:Basha\", \"City\": \"Hyd\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "c01",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "c02",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- c01: long (nullable = true)\n |-- c02: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "jsonString='{\"Id\":\"2\",\"Name\":\"Maheer:Basha\", \"City\": \"Hyd\"}'\n",
    "data= [(2,jsonString)]\n",
    "cols=[\"c01\", \"c02\"]\n",
    "df=spark.createDataFrame(data,cols)\n",
    "df.display()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea3ca2f5-d84d-4655-9f99-3b1f3d056cdb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>c07</th><th>c08</th></tr></thead><tbody><tr><td>MaheerBasha\", \"City\": \"Hyd\"}</td><td>{\"Id\":\"2\",\"Name\":\"MaheerBasha\", \"City\": \"Hyd\"}</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "MaheerBasha\", \"City\": \"Hyd\"}",
         "{\"Id\":\"2\",\"Name\":\"MaheerBasha\", \"City\": \"Hyd\"}"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "c07",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "c08",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, concat, concat_ws\n",
    "df1=df.withColumn(\"c03\",split(df.c02, '\"Name\":\"')[0])\\\n",
    ".withColumn(\"c04\",lit('\"Name\":\"'))\\\n",
    ".withColumn(\"c05\", split(col(\"c02\"),'\"Name\":\"')[1])\\\n",
    ".withColumn(\"c06\", split(\"c05\",':',2))\\\n",
    ".withColumn(\"c07\",concat_ws('',col(\"c06\")))\\\n",
    ".withColumn(\"c08\", concat(col(\"c03\"),col(\"c04\"),col(\"c07\"))).select(col(\"c07\"),col(\"c08\"))\n",
    "df1.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Remove double quotes from value of json string using PySpark",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
