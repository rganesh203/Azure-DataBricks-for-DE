{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOi8FAkWQKiCg+haQ6k6OYG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rganesh203/Pyspark/blob/main/Pyspark_Installation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #install java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# install spark (change the version number if needed)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# unzip the spark file to the current folder\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "# set your spark folder to your system path environment.\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "# install findspark using pip\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "B3Tt-JAxhkpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()"
      ],
      "metadata": {
        "id": "h6z4AlKCiGgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "fDCwRDejd3_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame, SparkSession\n",
        "from typing import List\n",
        "import pyspark.sql.types as T\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"Our First Spark example\") \\\n",
        "       .getOrCreate()"
      ],
      "metadata": {
        "id": "OeXk9HIRiD6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "X5DJFfE3A3rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.uiWebUrl"
      ],
      "metadata": {
        "id": "WB6oMVC7eMQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pyspark.__version__)\n"
      ],
      "metadata": {
        "id": "wdFPFUOFiV4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def create_dataframe(input_string):\n",
        "    # Initialize a Spark session\n",
        "    spark = SparkSession.builder.appName(\"StringToDataFrame\").getOrCreate()\n",
        "    # Convert the input string into an RDD and split lines\n",
        "    lines_rdd = spark.sparkContext.parallelize(input_string.splitlines())\n",
        "    # Split the header line and create a DataFrame from the RDD, skipping the header\n",
        "    header = lines_rdd.first()\n",
        "    data_rdd = lines_rdd.filter(lambda line: line != header)\n",
        "    header_columns = header.split(\",\")\n",
        "    data_df = data_rdd.map(lambda line: line.split(\",\")).toDF(header_columns)\n",
        "    # Filter out rows with \"NULL\" values in the \"age\" column\n",
        "    filtered_df = data_df.filter(~col(\"age\").isin(\"NULL\"))\n",
        "    return filtered_df\n",
        "input_string = \"id,name,age,score\\n1,Jack,NULL,12\\n17,Betty,28,11\"\n",
        "dataframe = create_dataframe(input_string)\n",
        "dataframe.show()\n"
      ],
      "metadata": {
        "id": "ZyzI-hr01Nho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "\n",
        "def create_dataframe(input_string):\n",
        "    # Initialize a Spark session\n",
        "    spark = SparkSession.builder.appName(\"StringToDataFrame\").getOrCreate()\n",
        "\n",
        "    # Split the input string into lines and create an RDD\n",
        "    rdd = spark.sparkContext.parallelize(input_string.split(\"\\n\"))\n",
        "\n",
        "    # Define a function to filter out rows with \"NULL\"\n",
        "    def filter_null_rows(row):\n",
        "        return row != \"NULL\"\n",
        "\n",
        "    # Apply the function and create a DataFrame\n",
        "    filtered_rdd = rdd.filter(filter_null_rows)\n",
        "    df = filtered_rdd.map(lambda x: tuple(x.split(','))).toDF(['col1', 'col2'])\n",
        "\n",
        "    # Split the input string into lines\n",
        "    lines = input_string.strip().split('\\n')\n",
        "\n",
        "    # Extract headers and data\n",
        "    headers = lines[0].split(',')\n",
        "    data_lines = lines[1:]\n",
        "\n",
        "    # Create a list of Row objects\n",
        "    rows = [Row(**dict(zip(headers, line.split(',')))) for line in data_lines]\n",
        "\n",
        "    # Create a DataFrame from the list of Row objects\n",
        "    df = spark.createDataFrame(rows)\n",
        "\n",
        "    return df\n",
        "\n",
        "input_string = \"header,header\\nANNUL,ANNULLED\\nnull,NILL\\nNULL,NULL\"\n",
        "dataframe = create_dataframe(input_string)\n",
        "dataframe.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "KA1M1syBjmxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content"
      ],
      "metadata": {
        "id": "E_pz9vn27f8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()\n",
        "def create_dataframe_from_string(input_string):\n",
        "    lines = input_string.split('\\n')\n",
        "    columns = lines[0].split(',')\n",
        "    data_rows = []\n",
        "    for line in lines[1:]:\n",
        "        values = line.split(',')\n",
        "        data_rows.append(Row(**{columns[i]: values[i] for i in range(len(columns))}))\n",
        "    rdd = spark.sparkContext.parallelize(data_rows)\n",
        "    df = spark.createDataFrame(rdd)\n",
        "    return df\n",
        "S = \"country,population,area\\nUK,67m,242000km2\"\n",
        "result_df = create_dataframe_from_string(S)\n",
        "result_df.show()\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "__JpJEW--ucN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from io import StringIO\n",
        "import csv\n",
        "\n",
        "def process_input(input_str, skip_header=True):\n",
        "    spark = SparkSession.builder.appName(\"DefectiveRowsRemoval\").getOrCreate()\n",
        "\n",
        "    # Convert the input string to an RDD of lines\n",
        "    lines_rdd = spark.sparkContext.parallelize(input_str.split('\\n'))\n",
        "\n",
        "    # Skip the header if required\n",
        "    if skip_header:\n",
        "        header = lines_rdd.first()\n",
        "        lines_rdd = lines_rdd.filter(lambda line: line != header)\n",
        "\n",
        "    # Split each line into columns based on comma separator\n",
        "    split_rdd = lines_rdd.map(lambda line: next(csv.reader(StringIO(line))))\n",
        "\n",
        "    # Filter out the rows with any null values\n",
        "    filtered_rdd = split_rdd.filter(lambda columns: all(col is not None and col.lower() != \"NULL\" for col in columns))\n",
        "\n",
        "    # Convert the resulting RDD back to a DataFrame\n",
        "    schema = split_rdd.first()  # Assuming the first row contains column names\n",
        "    df = spark.createDataFrame(filtered_rdd, schema=schema)\n",
        "\n",
        "    return df\n",
        "\n",
        "input_str = \"header,header\\nANNUL,ANNULLED\\nnull,NILL\\nNULL,NULL\"\n",
        "result_df = process_input(input_str, skip_header=True)\n",
        "result_df.show()\n"
      ],
      "metadata": {
        "id": "Rs9zmBZMJCCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}