{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80480629-dd65-42e3-bfa3-28fa845cdb14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üöÄ PySpark coalesce() Function\n",
    "üîπ What is coalesce() in PySpark?\n",
    "\n",
    "The coalesce() function is used to reduce the number of partitions in a DataFrame without shuffling the data across the cluster.\n",
    "It‚Äôs primarily used to optimize performance before writing data to storage, such as Parquet, Delta, or CSV.\n",
    "\n",
    "‚ö° Use coalesce() when you want to reduce partitions (e.g., from 10 ‚Üí 2).\n",
    "‚ö†Ô∏è Don‚Äôt use it to increase partitions ‚Äî use repartition() for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43f7d46b-0343-4f96-bb0f-7a7aba4d3c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "DataFrame.coalesce(numPartitions)\n",
    "numPartitions ‚Üí Target number of partitions (must be less than current count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f43091c2-46cb-4716-8e7d-6091d08b1cb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>emp_id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Alice</td><td>3000</td></tr><tr><td>2</td><td>Bob</td><td>4000</td></tr><tr><td>3</td><td>Cathy</td><td>2500</td></tr><tr><td>4</td><td>David</td><td>3500</td></tr><tr><td>5</td><td>Eve</td><td>2000</td></tr><tr><td>6</td><td>Frank</td><td>2800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Alice",
         3000
        ],
        [
         2,
         "Bob",
         4000
        ],
        [
         3,
         "Cathy",
         2500
        ],
        [
         4,
         "David",
         3500
        ],
        [
         5,
         "Eve",
         2000
        ],
        [
         6,
         "Frank",
         2800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "emp_id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial partitions: 8\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    (1, \"Alice\", 3000),\n",
    "    (2, \"Bob\", 4000),\n",
    "    (3, \"Cathy\", 2500),\n",
    "    (4, \"David\", 3500),\n",
    "    (5, \"Eve\", 2000),\n",
    "    (6, \"Frank\", 2800)\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, [\"emp_id\", \"name\", \"salary\"])\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "df.display()\n",
    "\n",
    "# Check initial partitions\n",
    "print(\"Initial partitions:\", df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f6e7193-ff19-4ff1-9bd6-48e9a122f0e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Apply coalesce() to Reduce Partitions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions after coalesce: 2\n"
     ]
    }
   ],
   "source": [
    "# Reduce partitions to 2\n",
    "df_coalesced = df.coalesce(2)\n",
    "\n",
    "print(\"Partitions after coalesce:\", df_coalesced.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b69ff1-8859-44f2-a7f8-b3d0b58eab78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#üíæ Use Case: Writing Optimized Output\n",
    "\n",
    "#When saving small data to storage, reducing partitions helps avoid many tiny files.\n",
    "\n",
    "# Example: write to Parquet with optimized partition count\n",
    "df_coalesced.write.mode(\"overwrite\").parquet(\"/tmp/optimized_output/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45921e7d-1e30-4c17-916b-a1ceb97c2a18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚öñÔ∏è coalesce() vs repartition()\n",
    "\n",
    "| Feature                 | `coalesce()`        | `repartition()`           |\n",
    "| ----------------------- | ------------------- | ------------------------- |\n",
    "| Shuffles Data           | ‚ùå No                | ‚úÖ Yes                     |\n",
    "| Can Increase Partitions | ‚ùå No                | ‚úÖ Yes                     |\n",
    "| Best For                | Reducing partitions | Increasing partitions     |\n",
    "| Performance             | Faster (no shuffle) | Slower (shuffle involved) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d5db406-a89e-4788-a739-013b298b7cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###  Reduce partitions using coalesce\n",
    "df_coalesce = df.coalesce(2)\n",
    "print(\"Coalesce partitions:\", df_coalesce.rdd.getNumPartitions())\n",
    "\n",
    "###  Repartition data (full shuffle)\n",
    "df_repartition = df.repartition(4)\n",
    "print(\"Repartition partitions:\", df_repartition.rdd.getNumPartitions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1795320b-eb20-4460-859e-ad5240dc506f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üí° Tips\n",
    "\n",
    "Use coalesce() before writing to reduce output files.\n",
    "\n",
    "Avoid reducing to 1 unless you truly need a single output file.\n",
    "\n",
    "Combine with .cache() or .persist() if reusing the coalesced DataFrame."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "56. PySpark coalesce() Function Tutorial - Optimize Partitioning for Faster Spark Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}