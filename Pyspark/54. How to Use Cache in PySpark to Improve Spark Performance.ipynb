{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24353dc1-c83f-4cff-8e38-608b562fcf8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîπ Why Use Cache in PySpark?\n",
    "\n",
    "When you perform transformations in Spark, they are lazy ‚Äî meaning Spark doesn‚Äôt compute anything until an action (like .show(), .count(), .collect()) is called.\n",
    "If the same DataFrame is reused multiple times, Spark recomputes it every time ‚Äî which can be expensive.\n",
    "\n",
    "üëâ Caching tells Spark to keep the results of a DataFrame in memory (or disk) after the first computation, so subsequent actions are much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1595c3cd-4c8e-4dc2-bd7c-df520b079e92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîπ Methods for Caching in PySpark\n",
    "Method\tStorage Level\tDescription\n",
    "df.cache()\tMEMORY_ONLY\tDefault caching ‚Äî stores DataFrame in memory only.\n",
    "df.persist()\tCustom\tLets you specify the storage level (e.g., MEMORY_AND_DISK).\n",
    "unpersist()\t‚Äî\tRemoves the DataFrame from cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3de5e13d-aa52-412a-86c5-e28215b89a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#üîπ Example: Using Cache in PySpark\n",
    "\n",
    "#Here‚Äôs a sample you can run in Databricks or any PySpark environment.\n",
    "\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "# ‚úÖ Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"Electronics\", 1000),\n",
    "    (2, \"Electronics\", 1500),\n",
    "    (3, \"Furniture\", 800),\n",
    "    (4, \"Clothing\", 400),\n",
    "    (5, \"Clothing\", 600)\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"category\", \"amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f89a065c-95bb-4944-97c0-9d495d9c0771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Action:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_sales</th></tr></thead><tbody><tr><td>Electronics</td><td>2500</td></tr><tr><td>Clothing</td><td>1000</td></tr><tr><td>Furniture</td><td>800</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Electronics",
         2500
        ],
        [
         "Clothing",
         1000
        ],
        [
         "Furniture",
         800
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚úÖ Perform a transformation\n",
    "sales_by_category = df.groupBy(\"category\").agg(_sum(\"amount\").alias(\"total_sales\"))\n",
    "\n",
    "# ‚úÖ Cache the transformed DataFrame\n",
    "sales_by_category.cache()\n",
    "\n",
    "# ‚ö° First action triggers computation and caches the result\n",
    "print(\"Initial Action:\")\n",
    "sales_by_category.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e45db491-7d85-400d-93a7-d0cdbce9e9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing Cached DataFrame:\nOut[4]: DataFrame[category: string, total_sales: bigint]"
     ]
    }
   ],
   "source": [
    "# ‚ö° Second action reuses cache ‚Äî much faster\n",
    "print(\"Reusing Cached DataFrame:\")\n",
    "sales_by_category.count()\n",
    "\n",
    "# ‚úÖ Remove from cache if not needed anymore\n",
    "sales_by_category.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f078661-ab15-4397-ab89-7c70902ba8b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: StorageLevel(True, False, False, False, 1)"
     ]
    }
   ],
   "source": [
    "#üîπ Verify Cache Storage Level\n",
    "#You can check the storage level like this:\n",
    "sales_by_category.storageLevel\n",
    "#Output (for cache()):\n",
    "StorageLevel(True, False, False, False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf41338-1fd2-4d90-9c42-cbd00096e556",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: DataFrame[category: string, total_sales: bigint]"
     ]
    }
   ],
   "source": [
    "#üîπ Using persist() for More Control\n",
    "#You can persist with a different storage level, for example:\n",
    "from pyspark import StorageLevel\n",
    "# Store in both memory and disk\n",
    "sales_by_category.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "#This is safer when the dataset is too large to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3710ffa7-d602-4fd8-9321-793a1c44af4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üîπ When to Use Cache / Persist\n",
    "\n",
    "‚úÖ Use cache() or persist() when:\n",
    "\n",
    "You reuse the same DataFrame multiple times in a job.\n",
    "\n",
    "You perform iterative algorithms (like ML model training or graph computations).\n",
    "\n",
    "You materialize intermediate results that are expensive to recompute.\n",
    "\n",
    "‚ùå Avoid caching when:\n",
    "\n",
    "The DataFrame is used only once.\n",
    "\n",
    "The DataFrame is too large to fit in memory.\n",
    "\n",
    "### üîπ Bonus: Check Cached Tables in Spark UI\n",
    "\n",
    "When running in Databricks or Spark UI:\n",
    "\n",
    "Go to the Storage tab.\n",
    "\n",
    "You‚Äôll see all cached DataFrames, their memory size, and storage level."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "54. How to Use Cache in PySpark to Improve Spark Performance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}