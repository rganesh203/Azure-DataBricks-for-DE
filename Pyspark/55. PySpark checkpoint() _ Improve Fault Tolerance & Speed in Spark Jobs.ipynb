{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf42a22-09f4-43a6-a47b-77b1d6901814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "üîç Why Use checkpoint()?\n",
    "\n",
    "Fault tolerance: If a job fails, Spark can restart from the checkpoint instead of recomputing from the beginning.\n",
    "\n",
    "Performance: Long transformation chains create long DAGs; checkpointing truncates the lineage, reducing recomputation overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26df3465-60ad-41ad-a082-dd4e531b5310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial DataFrame:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category</th><th>amount</th></tr></thead><tbody><tr><td>1</td><td>A</td><td>100</td></tr><tr><td>2</td><td>B</td><td>200</td></tr><tr><td>3</td><td>A</td><td>150</td></tr><tr><td>4</td><td>B</td><td>50</td></tr><tr><td>5</td><td>C</td><td>300</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "A",
         100
        ],
        [
         2,
         "B",
         200
        ],
        [
         3,
         "A",
         150
        ],
        [
         4,
         "B",
         50
        ],
        [
         5,
         "C",
         300
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#‚úÖ Example: Using checkpoint() in PySpa\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "# Set checkpoint directory (should be HDFS, DBFS, or similar persistent storage in production)\n",
    "spark.sparkContext.setCheckpointDir(\"/tmp/spark_checkpoints\")\n",
    "\n",
    "# Create a sample DataFrame\n",
    "data = [\n",
    "    (1, \"A\", 100),\n",
    "    (2, \"B\", 200),\n",
    "    (3, \"A\", 150),\n",
    "    (4, \"B\", 50),\n",
    "    (5, \"C\", 300)\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"category\", \"amount\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Initial DataFrame:\")\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1864e69d-f680-427f-8032-75db27e6ee96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before checkpoint:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_amount</th></tr></thead><tbody><tr><td>A</td><td>295.0</td></tr><tr><td>B</td><td>295.0</td></tr><tr><td>C</td><td>354.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         295.0
        ],
        [
         "B",
         295.0
        ],
        [
         "C",
         354.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform a few transformations\n",
    "df_transformed = df.withColumn(\"amount_with_tax\", col(\"amount\") * 1.18)\n",
    "\n",
    "df_aggregated = df_transformed.groupBy(\"category\") \\\n",
    "    .agg(spark_sum(\"amount_with_tax\").alias(\"total_amount\"))\n",
    "\n",
    "print(\"Before checkpoint:\")\n",
    "df_aggregated.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fda5b7c-54af-4d8b-b8b6-1ae78378adc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After checkpoint (lineage truncated):\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_amount</th></tr></thead><tbody><tr><td>A</td><td>295.0</td></tr><tr><td>B</td><td>295.0</td></tr><tr><td>C</td><td>354.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         295.0
        ],
        [
         "B",
         295.0
        ],
        [
         "C",
         354.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checkpoint to truncate lineage\n",
    "df_checkpointed = df_aggregated.checkpoint(eager=True)  # eager=True triggers immediate checkpoint\n",
    "\n",
    "print(\"After checkpoint (lineage truncated):\")\n",
    "df_checkpointed.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45668b1e-56ef-4fce-8302-c5c47f7fcd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final output:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_amount</th><th>discounted_amount</th></tr></thead><tbody><tr><td>A</td><td>295.0</td><td>265.5</td></tr><tr><td>B</td><td>295.0</td><td>265.5</td></tr><tr><td>C</td><td>354.0</td><td>318.6</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "A",
         295.0,
         265.5
        ],
        [
         "B",
         295.0,
         265.5
        ],
        [
         "C",
         354.0,
         318.6
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "discounted_amount",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform further transformations safely on the checkpointed DataFrame\n",
    "df_final = df_checkpointed.withColumn(\"discounted_amount\", col(\"total_amount\") * 0.9)\n",
    "\n",
    "print(\"Final output:\")\n",
    "df_final.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c46e868-9547-4c84-a57e-c0ff4e510cf6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚öôÔ∏è Key Points\n",
    "\n",
    "| Parameter                             | Description                                                                                                  |\n",
    "| ------------------------------------- | ------------------------------------------------------------------------------------------------------------ |\n",
    "| `sparkContext.setCheckpointDir(path)` | Sets where checkpoint data is stored. Should be a **reliable** filesystem (HDFS, DBFS, ADLS, S3, etc.).      |\n",
    "| `.checkpoint(eager=True)`             | Forces checkpoint immediately instead of waiting for an action.                                              |\n",
    "| `.persist()` vs `.checkpoint()`       | `persist()` caches data in memory/disk but keeps lineage; `checkpoint()` **cuts lineage** and saves to disk. |\n",
    "\n",
    "üß† When to Use Checkpoint\n",
    "\n",
    "Use checkpoint() when:\n",
    "\n",
    "You have iterative algorithms (e.g., PageRank, graph algorithms).\n",
    "\n",
    "You have very long lineage chains (many transformations).\n",
    "\n",
    "You need fault recovery from a stable intermediate point."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "55. PySpark checkpoint() : Improve Fault Tolerance & Speed in Spark Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}