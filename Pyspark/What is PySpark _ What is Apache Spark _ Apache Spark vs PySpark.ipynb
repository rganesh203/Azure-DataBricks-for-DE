{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d61669b-0d3d-44d8-9d58-27768b06d4c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let’s dive deeper into what Apache Spark and PySpark are, their architecture, components, and how they compare.\n",
    "\n",
    "\uD83D\uDD25 What is Apache Spark?\n",
    "Apache Spark is an open-source, distributed computing system designed for fast computation on large-scale data. It was developed at UC Berkeley’s AMPLab and later donated to the Apache Software Foundation.\n",
    "\n",
    "⚙️ Core Features of Apache Spark:\n",
    "Speed: Spark processes data in-memory, which makes it up to 100x faster than traditional MapReduce systems (like Hadoop).\n",
    "\n",
    "Distributed Processing: It splits tasks across multiple machines (nodes) for parallel processing.\n",
    "\n",
    "Ease of Use: APIs available in Scala, Java, Python (PySpark), and R.\n",
    "\n",
    "Unified Engine: Supports batch processing, streaming, SQL queries, machine learning, and graph processing within a single framework.\n",
    "\n",
    "\uD83D\uDD27 Key Components of Apache Spark:\n",
    "Component\tDescription\n",
    "Spark Core\tThe base engine for scheduling, distributing, and monitoring jobs.\n",
    "Spark SQL\tModule for structured data processing with SQL queries and DataFrames.\n",
    "Spark Streaming\tHandles real-time data streams (like Kafka, Flume).\n",
    "MLlib\tMachine Learning library built on top of Spark.\n",
    "GraphX\tFor graph computation (nodes, edges, relationships).\n",
    "\n",
    "\uD83D\uDC0D What is PySpark?\n",
    "PySpark is the Python API for Apache Spark, allowing you to harness Spark’s capabilities using Python, one of the most popular programming languages in data science and analytics.\n",
    "\n",
    "✨ Why use PySpark?\n",
    "Leverages the power of Spark with the simplicity of Python.\n",
    "\n",
    "Seamlessly integrates with Python libraries like pandas, NumPy, matplotlib, and ML frameworks such as scikit-learn.\n",
    "\n",
    "Allows data scientists to build scalable ML pipelines using Spark’s MLlib.\n",
    "\n",
    "\uD83C\uDFD7️ Architecture Overview\n",
    "\uD83D\uDD39 Apache Spark:\n",
    "Driver Program: Your main application which defines transformations and actions on data.\n",
    "\n",
    "Cluster Manager: Allocates resources (can be YARN, Mesos, Kubernetes, or standalone).\n",
    "\n",
    "Executors: Run tasks on worker nodes.\n",
    "\n",
    "RDDs (Resilient Distributed Datasets): Immutable distributed collections of data.\n",
    "\n",
    "\uD83D\uDD39 PySpark:\n",
    "When you write PySpark code, it runs Python code on the driver side, but the actual execution logic (tasks) is translated into JVM bytecode and executed by Spark on the worker nodes.\n",
    "\n",
    "PySpark uses Py4J to interface between Python and the JVM (Java Virtual Machine).\n",
    "\n",
    "\uD83D\uDCCA Apache Spark vs PySpark\n",
    "Feature\tApache Spark (Scala/Java)\tPySpark (Python)\n",
    "Language\tScala/Java (native)\tPython\n",
    "Performance\tSlightly faster (native JVM)\tSlightly slower (uses Py4J bridge)\n",
    "Ease of Use\tVerbose syntax\tCleaner, more readable\n",
    "Community Support\tStrong, especially in enterprise\tVery strong in data science community\n",
    "Integration\tSeamless with Hadoop ecosystem\tSeamless with Python data tools\n",
    "Ideal For\tLow-level control, performance-critical jobs\tData analysis, ML, prototyping\n",
    "\n",
    "✅ Use Cases of PySpark\n",
    "Big Data ETL Pipelines: Clean, transform, and process massive datasets from various sources.\n",
    "\n",
    "Real-time Analytics: Streaming data processing from sensors, logs, or events.\n",
    "\n",
    "Machine Learning Pipelines: Scalable ML models using MLlib.\n",
    "\n",
    "Data Lakes: Integration with Delta Lake and lakehouse architecture.\n",
    "\n",
    "Ad-hoc Queries: Running SQL queries on huge datasets using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00199d08-8892-4077-9bbf-e5190a19c814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#\uD83D\uDCCC Example PySpark Code\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"ExampleApp\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform transformation\n",
    "df_filtered = df.filter(df['age'] > 30).select(\"name\", \"age\")\n",
    "\n",
    "# Show results\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7341c804-97ec-44ed-ac4c-8d217f023c2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\uD83D\uDE80 Summary\n",
    "Apache Spark is a fast and scalable general-purpose distributed computing system.\n",
    "\n",
    "PySpark brings the power of Spark to Python developers, bridging big data with Python's flexibility.\n",
    "\n",
    "PySpark is ideal for data scientists, analysts, and engineers looking to process large datasets using familiar tools.\n",
    "\n",
    "Let me know if you'd like an architecture diagram, example use cases, or want to run a sample project in PySpark.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "What is PySpark ? What is Apache Spark | Apache Spark vs PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}