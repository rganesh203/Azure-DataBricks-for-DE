{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72345544-8e90-42fe-8bc3-5b300aac1bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "PySpark DataFrame .to() function, schema reconciliation, and column reordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6a0cd9a-534e-4010-959b-5dac45f2fb38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. What is DataFrame .to() in PySpark?\n",
    "\n",
    "In PySpark, the .to() function is not a direct DataFrame method, but it is used in combination with certain APIs like:\n",
    "\n",
    ".toDF() → Converts RDD to DataFrame or renames columns.\n",
    "\n",
    ".toPandas() → Converts PySpark DataFrame to Pandas.\n",
    "\n",
    ".toJSON() → Converts DataFrame rows into JSON strings.\n",
    "\n",
    ".toLocalIterator() → Converts a DataFrame into a local Python iterator.\n",
    "\n",
    ".to() in DataFrameWriter → Used when writing to specific formats.\n",
    "\n",
    "However, since you also mentioned schema reconciliation and column reordering, I believe you're working with DataFrame operations like union, merge, write, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24e6623e-864a-4b46-b166-886501c9b2f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Schema Reconciliation in PySpark\n",
    "\n",
    "Schema reconciliation happens when two DataFrames with different column order, names, or data types are combined using operations like union, join, or write.\n",
    "\n",
    "Problem\n",
    "\n",
    "You have two DataFrames with same columns but different column order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5e84ed2-565a-4353-9ecc-492fe20b2737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType \n",
    "\n",
    "# Sample DataFrame 1\n",
    "data1 = [(1, \"Ganesh\", 5000), (2, \"Raj\", 7000)]\n",
    "schema1 = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "\n",
    "# Sample DataFrame 2 (different column order)\n",
    "data2 = [(\"Anil\", 3, 6000), (\"Kiran\", 4, 8000)]\n",
    "schema2 = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True)\n",
    "])\n",
    "df1 = spark.createDataFrame(data1, schema1)\n",
    "df2 = spark.createDataFrame(data2, schema2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13be66c6-eb73-40ed-ad0f-f894606a05d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Ganesh</td><td>5000</td></tr><tr><td>2</td><td>Raj</td><td>7000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ganesh",
         5000
        ],
        [
         2,
         "Raj",
         7000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>id</th><th>salary</th></tr></thead><tbody><tr><td>Anil</td><td>3</td><td>6000</td></tr><tr><td>Kiran</td><td>4</td><td>8000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Anil",
         3,
         6000
        ],
        [
         "Kiran",
         4,
         8000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.display()\n",
    "df2.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d8a006c-594a-48f2-8dc3-37cb98dfbc9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Without Schema Reconciliation\n",
    "df1.union(df2).show()\n",
    "\n",
    "\n",
    "❌ Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790e34ac-cd7a-488f-b416-c0525aee4911",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution → Use unionByName with allowMissingColumns=True"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>salary</th></tr></thead><tbody><tr><td>1</td><td>Ganesh</td><td>5000</td></tr><tr><td>2</td><td>Raj</td><td>7000</td></tr><tr><td>3</td><td>Anil</td><td>6000</td></tr><tr><td>4</td><td>Kiran</td><td>8000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ganesh",
         5000
        ],
        [
         2,
         "Raj",
         7000
        ],
        [
         3,
         "Anil",
         6000
        ],
        [
         4,
         "Kiran",
         8000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_reconciled = df1.unionByName(df2, allowMissingColumns=True)\n",
    "df_reconciled.display()\n",
    "#✅ Schema reconciled automatically — column order doesn't matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b073209-a354-476a-9479-7079f1ae4abb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Column Reordering in PySpark\n",
    "\n",
    "Sometimes, you want to reorder columns after reconciliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8bbd86-9f7c-474c-b743-2855eba36a7d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Approach 1 → Using select()"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>id</th><th>salary</th></tr></thead><tbody><tr><td>Ganesh</td><td>1</td><td>5000</td></tr><tr><td>Raj</td><td>2</td><td>7000</td></tr><tr><td>Anil</td><td>3</td><td>6000</td></tr><tr><td>Kiran</td><td>4</td><td>8000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Ganesh",
         1,
         5000
        ],
        [
         "Raj",
         2,
         7000
        ],
        [
         "Anil",
         3,
         6000
        ],
        [
         "Kiran",
         4,
         8000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_reordered = df_reconciled.select(\"name\", \"id\", \"salary\")\n",
    "df_reordered.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1bc8ff3-3e07-4a42-9ff1-ea020ecea77a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Approach 2 → Dynamically Reorder Columns"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>salary</th><th>name</th><th>id</th></tr></thead><tbody><tr><td>5000</td><td>Ganesh</td><td>1</td></tr><tr><td>7000</td><td>Raj</td><td>2</td></tr><tr><td>6000</td><td>Anil</td><td>3</td></tr><tr><td>8000</td><td>Kiran</td><td>4</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5000,
         "Ganesh",
         1
        ],
        [
         7000,
         "Raj",
         2
        ],
        [
         6000,
         "Anil",
         3
        ],
        [
         8000,
         "Kiran",
         4
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "desired_order = [\"salary\", \"name\", \"id\"]\n",
    "df_dynamic = df_reconciled.select([col for col in desired_order])\n",
    "df_dynamic.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ffe5c85-799a-4ab9-ab31-ed3cef469e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Converting Between DataFrames "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "623e5e0e-d2ad-495c-b8f8-88b9095744fe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4.1. RDD → DataFrame Using .toDF()"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th></tr></thead><tbody><tr><td>1</td><td>Ganesh</td></tr><tr><td>2</td><td>Raj</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Ganesh"
        ],
        [
         2,
         "Raj"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([(1, \"Ganesh\"), (2, \"Raj\")])\n",
    "df_from_rdd = rdd.toDF([\"id\", \"name\"])\n",
    "df_from_rdd.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d15e488-73c3-4978-87a7-ad3caa802e3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4.2. PySpark → Pandas Using .toPandas()"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    name  salary\n0   1  Ganesh    5000\n1   2     Raj    7000\n2   3    Anil    6000\n3   4   Kiran    8000\n"
     ]
    }
   ],
   "source": [
    "pdf = df_reconciled.toPandas()\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4147e53f-68b3-4a75-8895-199a9b0f7b48",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "4.3. DataFrame → JSON Using .toJSON()"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[12]: ['{\"id\":1,\"name\":\"Ganesh\",\"salary\":5000}',\n '{\"id\":2,\"name\":\"Raj\",\"salary\":7000}']"
     ]
    }
   ],
   "source": [
    "df_reconciled.toJSON().take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9283af65-bd22-4d95-a72a-3ee09179be6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Summary Table\n",
    "\n",
    "| **Function**     | **Purpose**                                     | **Example**                       |\n",
    "| ---------------- | ----------------------------------------------- | --------------------------------- |\n",
    "| `.toDF()`        | Convert RDD to DataFrame / rename cols          | `rdd.toDF([\"id\", \"name\"])`        |\n",
    "| `.toPandas()`    | Convert PySpark DataFrame → Pandas              | `df.toPandas()`                   |\n",
    "| `.toJSON()`      | Convert DataFrame rows → JSON strings           | `df.toJSON()`                     |\n",
    "| `.unionByName()` | Schema reconciliation when column order differs | `df1.unionByName(df2)`            |\n",
    "| `.select()`      | Column reordering                               | `df.select(\"name\",\"id\",\"salary\")` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1be3053f-4226-4e16-a42a-e9a215225ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Key Takeaways\n",
    "\n",
    "Use unionByName(..., allowMissingColumns=True) for schema reconciliation.\n",
    "\n",
    "Use select() or a dynamic list for column reordering.\n",
    "\n",
    "Use .toPandas(), .toJSON(), .toDF() as needed for conversions."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "42. PySpark DataFrame.to Function | Schema Reconciliation & Column Reordering",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}