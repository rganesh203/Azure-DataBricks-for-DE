{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c623ecaf-66e2-4cc5-a8f6-da15ff99089c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In PySpark, a DataFrame is a distributed collection of data organized into named columns. It's conceptually similar to a table in a relational database or a DataFrame in Pandas. PySpark DataFrames are designed to process large-scale data processing tasks across distributed computing environments.\n",
    "\n",
    "To create a DataFrame from static values in PySpark, you typically use the createDataFrame function from the pyspark.sql module. Here's a basic example of how you can create a DataFrame from static values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5449230e-c3b0-4eea-a674-f801a07fa9e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "# Define static data\n",
    "data = [\n",
    "    Row(id=1, name='Alice', age=25),\n",
    "    Row(id=2, name='Bob', age=30),\n",
    "    Row(id=3, name='Charlie', age=35)\n",
    "]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab720000-6987-4249-9e5c-2bfe35687be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#1. Creating DataFrame from a List of Tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86deb675-956b-4ff6-a69d-c8b876cd8248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data as a list of tuples\n",
    "data = [\n",
    "    (1, \"Alice\", 25),\n",
    "    (2, \"Bob\", 30),\n",
    "    (3, \"Charlie\", 35)\n",
    "]\n",
    "\n",
    "# Define the schema (column names)\n",
    "columns = [\"id\", \"name\", \"age\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b467f97-58e4-47ba-86c6-b7b4c7c79108",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Note:\n",
    "Each tuple represents a row.\n",
    "\n",
    "The columns list defines the DataFrame's schema.\n",
    "\n",
    "This method is great for quick testing and small, static datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d083ca4-0bda-40d0-b547-a19e46a89af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Creating DataFrame from List of Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9604c87-7d12-48b2-b984-e047892b7771",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define data as a list of dictionaries\n",
    "data = [\n",
    "    {\"id\": 1, \"name\": \"Alice\", \"age\": 25},\n",
    "    {\"id\": 2, \"name\": \"Bob\", \"age\": 30},\n",
    "    {\"id\": 3, \"name\": \"Charlie\", \"age\": 35}\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a650fd0b-713b-4e5c-9a41-c33e897a1613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Note:\n",
    "\n",
    "This method automatically infers the schema from the dictionary keys.\n",
    "\n",
    "Itâ€™s particularly useful when you're working with JSON-like structures or data from APIs."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "What is a DataFrame in PySpark? | How to create DataFrame from Static Values",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}