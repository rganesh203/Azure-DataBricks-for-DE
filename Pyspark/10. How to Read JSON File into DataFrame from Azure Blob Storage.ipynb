{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "363b94ee-90fb-47f3-8103-692015d3f8c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define Parameters"
    }
   },
   "outputs": [],
   "source": [
    "storage_account_name = \"yourstorageaccount\"\n",
    "container_name = \"yourcontainer\"\n",
    "file_path = \"path/to/yourfile.json\"\n",
    "\n",
    "# If using a SAS token\n",
    "sas_token = \"sv=2023-01-01&ss=b&srt=sco&sp=rl&se=2025-08-01T00:00:00Z&st=2025-07-20T00:00:00Z&spr=https&sig=XYZ\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8332bb5-a728-4fcd-ba15-acbe59304e40",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Build the ABFS or ABFSS Path"
    }
   },
   "outputs": [],
   "source": [
    "json_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/{file_path}\"\n",
    "#✅ Use abfss:// for secure access to ADLS Gen2 (or Blob with hierarchical namespace enabled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a5ab9fb-a741-4301-bdac-3253d20cb52b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set Spark Configuration for SAS Token Access"
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(f\"fs.azure.sas.{container_name}.{storage_account_name}.dfs.core.windows.net\", sas_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ff3edf0-767b-4dda-9add-6efd187d0751",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read JSON into DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(json_path)\n",
    "\n",
    "# Show the schema and some rows\n",
    "df.printSchema()\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6883764-0c9d-44f1-9e38-0e74177b017a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If you’re using Azure Blob Storage (classic, not Gen2), the endpoint might be .blob.core.windows.net, and you’ll need to use wasbs:// instead of abfss://. But for Gen2 (Data Lake), use abfss://.\n",
    "\n",
    "Use inferSchema only when needed for formats like CSV — JSON doesn’t require it."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "10. How to Read JSON File into DataFrame from Azure Blob Storage",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}