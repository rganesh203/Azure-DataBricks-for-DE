{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708cd160-7d4e-4b31-9d2d-cec235761c3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: ['Dravid,100,2000,3,4',\n 'Sachin,100,3000,4,5',\n 'Sangakara,200,5000,6,7',\n 'Abd,400,10000,5,9']"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()\n",
    "df = spark.sparkContext.parallelize(['Dravid,100,2000,3,4', 'Sachin,100,3000,4,5', 'Sangakara,200,5000,6,7', 'Abd,400,10000,5,9'])\n",
    "df.collect()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d06c5117-dc22-4e69-8404-243fbbac1d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: [['Dravid', '100', '2000', '3', '4'],\n ['Sachin', '100', '3000', '4', '5'],\n ['Sangakara', '200', '5000', '6', '7'],\n ['Abd', '400', '10000', '5', '9']]"
     ]
    }
   ],
   "source": [
    "rdd1=df.map(lambda row:row.split(\",\"))\n",
    "rdd1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69ab9244-5965-4b47-bc15-024bee5400a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[8]: [('100', <pyspark.resultiterable.ResultIterable at 0x7fbc8f96e520>),\n ('200', <pyspark.resultiterable.ResultIterable at 0x7fbc8fa87760>),\n ('400', <pyspark.resultiterable.ResultIterable at 0x7fbc8fa87ee0>)]"
     ]
    }
   ],
   "source": [
    "rdd2=rdd1.groupBy(lambda row:row[1])\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "995a8d28-b130-4f57-9571-6688b4aebd36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['DRAVID', 100, 2000, 3, 4], ['SACHIN', 100, 3000, 4, 5], ['SANGAKARA', 200, 5000, 6, 7], ['ABD', 400, 10000, 5, 9]]\n"
     ]
    }
   ],
   "source": [
    "def funcl(row):\n",
    "    list_new = []\n",
    "    for i in range(0,len(row)):\n",
    "        if i==0:\n",
    "            list_new.append(row[0].upper())\n",
    "        else:\n",
    "            list_new.append(int(row[i]))\n",
    "    return list_new\n",
    "rdd1 = df.map(lambda row: row.split(','))\n",
    "#print(rdd1.collect())\n",
    "\n",
    "rdd2 = rdd1.map(funcl)\n",
    "print(rdd2.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "665c207b-5557-4af1-8166-16279665f5cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Interview Question",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
